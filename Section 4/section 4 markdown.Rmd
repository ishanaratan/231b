---
title: "231B - Section 4"
author: "Chris Carter"
date: "February 12, 2019"
output: slidy_presentation
incremental: true
---

```{r, echo=FALSE}

rm(list=ls())
set.seed(94705)

```


```{r}
# Libraries
library(MASS)

# sourcing plotting functions
functions <-  "https://raw.githubusercontent.com/unc421/231b/master/Functions/functions.R"
source(functions)

```



Regression as an algorithm
============================================

Until last week, we were thinking about regression as an algorithm, a "formula" to minimize the sum of the squared residuals. 

> - We minimize $$e' e$$ by taking the derivative with respect to $\hat{\beta}$ and setting it equal to $0$ (we can rewrite $e$ as $e = Y-\hat{Y} = Y-X\hat{\beta}$). So we get

$$ \hat{\beta}  = \big( X ' X \big)^{-1} X' Y$$


> - What are dimensions on $X$, $Y$, and $\hat{\beta}$?

---

Are we making any assumptions about the distribution of $Y$? $X$? $e$?

> - No

So what do we need?

> - We need $X$ to be full rank
> - Why?

What are two properties of regression?

> - $X\perp e$ and $\bar{e}=0$

> - Why?

When $X$ is full rank, is $\hat{\beta}$ biased?

> - It makes no sense to think about bias in this context because $\hat{\beta}$ is not a random variable!


Assumptions on the regression model
=================================================

- The data on $Y$ are observed values of $X \beta + \epsilon$

- The $\epsilon_i$ are independent and identically distributed, with mean $0$ and variance $\sigma^2$

- The $\epsilon_i$ are independent of $X$

(... and we still need $X$ to be full rank!)

> - What about causation?
> - (To be continued...)

Agenda for today
===============================================================

1. Fitting regression SEs and t-test

2. Monte Carlo simulation: unbiasedness and omitted variable bias

---

An `R` digression 
===============================================================

- Loading your functions using `source( )`

- The `replicate( )` command for simulations



1. Fitting regression SEs and t-test
===============================================================

First we will generate some data following the multivariate regression model, this will be our data generating process (DGP).

```{r, tidy = TRUE}
#generating variation in x1 and x2 
#(note this does not mean we are treating them as random variables).
x1 <- runif(80, -50, 50)
x2 <- rnorm(80, 5, 20)

epsilon <- rnorm(80, 0, 16)

Y <- 14 + 8 * x1 + 3 * x2 + epsilon
```

What do each of these distributions look like? 

Where is $\sigma$ here? 

What parts of this DGP address each of the assumptions of the regression model?

---

Using matrix manipulation, calculate the regression residuals. Show the average of the residuals is zero and find $\hat{\sigma}$ based on sum of squared residuals, divided by $n-p$.

The first thing we will need here is to build the matrix for X.



---

How do we do this?

```{r, echo = FALSE}
X <- cbind(1, x1, x2)
```
How do we get the residuals now?

We know $e=Y-\hat{Y}=Y-X\hat{\beta}$. So we need to calculate $\hat{\beta}$ first. We did this last section:

```{r, echo = FALSE, results = 'hide'}
betas <- solve(t(X)%*%X) %*% t(X)%*%Y

e <- Y - X %*% betas

```
---

How do we do this?

```{r}
X <- cbind(1, x1, x2)
```
How do we get the residuals now?

We know $e=Y-\hat{Y}=Y-X\hat{\beta}$. So we need to calculate $\hat{\beta}$ first. How do we do that?

```{r, echo = FALSE}
betas <- solve(t(X)%*%X) %*% t(X)%*%Y

e <- Y - X %*% betas

```

---

How do we do this?

```{r}
X <- cbind(1, x1, x2)
```
How do we get the residuals now?

We know $e=Y-\hat{Y}=Y-X\hat{\beta}$. So we need to calculate $\hat{\beta}$ first. How do we do that?

```{r}
betas <- solve(t(X)%*%X) %*% t(X)%*%Y

e <- Y - X %*% betas

betas 

```

---

Mean of residuals and $\hat{\sigma}$.

```{r, results = 'hide'}
round(mean(e), 4)
```

```{r}
hat_sigma2 <- sum(t(e)%*%e) / (nrow(X)-length(betas))
hat_sigma <- sqrt(hat_sigma2)

hat_sigma
```

---

c. Find estimated standard errors from $\hat{\sigma^2}(X'X)^{-1}$. (What is this called?)

```{r}

var_hat_beta <- hat_sigma2 * solve((t(X)%*%X)) # Why do we use * instead of %*% here?
var_hat_beta 
```

---

What elements of this matrix do we care about?

```{r}
se_hat_beta <- sqrt(diag(var_hat_beta))
se_hat_beta

```

---

d. Conduct t-test based on $\hat{\beta}/\hat{SE}$. 

```{r, echo = FALSE, results = 'hide'}

t_stats <- betas/se_hat_beta
t_stats
```


---

d. Conduct t-test based on $\hat{\beta}/\hat{SE}$. 

```{r}

t_stats <- betas/se_hat_beta
t_stats
```

---

```{r}
for (i in 1:3){
  
  p_val <- pt(abs(t_stats[i]), df=(nrow(X)-length(betas)), ncp=0, lower.tail = F) + 
    pt(-abs(t_stats[i]), df=(nrow(X)-length(betas)), ncp=0, lower.tail = T)
  
  print(p_val)
}

```

Are these one or two-tailed tests?


2. Monte Carlo simulation: unbiasedness and omitted variable bias
===============================================================
  
To prove unbiasedness, we need to show that $E(\hat{\beta})=\beta$.

$$E\big[ \hat{\beta} \mid X \big] = E\big[ (X'X)^{-1}X'Y \mid X \big]$$

Say, however, that the real model is $Y=X\beta+Z\mu+\epsilon$. Then when we substitute for $Y$ we have:

$$\begin{array}
\[E\big[ \hat{\beta} \mid X \big] & = E\big[ (X'X)^{-1}X'(X\beta+Z\mu+\epsilon)  \mid X \big] \\
& = E\big[ (X'X)^{-1}X'X\beta+ (X'X)^{-1}X'Z\mu + (X'X)^{-1}X'\epsilon  \mid X \big] \\
& = E\big[ (X'X)^{-1}X'X\beta  \mid X \big]+ E\big[(X'X)^{-1}X'Z\mu  \mid X \big] + E\big[ (X'X)^{-1}X'\epsilon  \mid X \big] \\
& = E\big[ \beta  \mid X \big] + E\big[(X'X)^{-1}X'Z\mu  \mid X \big] + (X'X)^{-1}X'E\big[ \epsilon  \mid X \big] \\
& = \beta + \mu(X'X)^{-1}E(X'Z|X) + (X'X)^{-1}X'0 \\
& = \beta + \mu(X'X)^{-1}E(X'Z|X) 
\end{array}$$
  
---  
  
Let's take a closer look at $\mu(X'X)^{-1}E(X'Z|X)$ 

> - When $\mu$ is zero, the expression is $0$ and our regression of $Y$ on $X$ allows us to estimate \beta without bias. What does this mean?

> - When $X'Z=0$, the correlation between $X$ and $Z$ and the entire expression is also $0$.

> - What happens when $X'Z \neq 0$? Can we know anything about the direction or magnitude of the bias?
  
---  
  
We will program a simulation to see this. We will need to write a function that takes the following arguments and returns the regression coefficients for the regression we indicate:

- X matrix (more about this later)

- betas: true parameters of the model. Here, $\beta_{0} = 1$, $\beta_{1} = 2$, $\beta_{2} = 4$, $\beta_{3} = 3$

- residuals from other model (e)

- An omitted variable indicator: should we omit the third variable in the regression?

---
  
```{r, eval = FALSE}

ovb_sim <- function(X, betas, e, omitted=FALSE){
  
  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  # note that we are adding column of 1s for the intercept
  y <- cbind(1, X) %*% betas + sample(e, 100, replace=F)
 
  # Run a regression of the simulated y on the simulated X 
  # with the option of omitting x2 or not
 
  # Extract the estimated coefficients
  coefs <- 
  
  # Return the coefficients
  return(coefs)
}
```

---
  
```{r}

ovb_sim <- function(X, betas, e, omitted=FALSE){
  
  # Create the simulated y by
  # adding together the systematic and stochastic
  # components, according to the true model
  # note that we are adding column of 1s for the intercept
  y <- cbind(1, X) %*% betas + sample(e, 100, replace=F)
 
  # Run a regression of the simulated y on the simulated X 
  # with the option of omitting x2 or not
  if (omitted==FALSE) res <- lm(y~X) # complete model
  if (omitted==TRUE) res <- lm(y~X[,c(1,2)]) # omitting the third variable
  
  # Extract the estimated coefficients
  coefs <- res$coefficients
  
  # Return the coefficients
  return(coefs)
}
```

---
  
We will look at four different cases: 
  
1. The three independent variables belong in the real model and they are correlated -- and we specify the right regression.

2. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$

---

3. The three independent variables belong in the real model and they are correlated (negative covariance) -- our regression omits $x_3$

4. The three independent variables belong in the real model but they are not correlated -- our regression omits $x_3$

---

First we will need to create the matrices with the variances and covariances of $X$.

> - What is the key thing to consider when we build these matrices?

---
  
```{r}

# True variance-covariance matrices
# Here the off-diagonal elements are zero, so the variables are not correlated
# with each other
SigmaX_without <- matrix(c( 2,  0,  0,
                            0,  2,  0,
                            0,  0,  2 ), nrow=3, 
                         ncol=3, byrow=TRUE)
# Now in these the off-diagonal elements are non-zero (the variables are correlated)
SigmaX_with_positive <- matrix(c( 1,  .5,  .5,
                                 .5,   1,  .5,
                                 .5,  .5,   1 ), 
                               nrow=3, ncol=3, 
                               byrow=TRUE)

SigmaX_with_negative <- matrix(c(  1,  -.5,  -.5,
                                 -.5,    1,  -.5,
                                 -.5,  -.5,    1 ), 
                               nrow=3, ncol=3, 
                               byrow=TRUE)


```

---

And now we can use this to generate the `X` matrices (note we use `mvrnorm()`)

```{r}

# Draw the simulated covariates from their true
# multivariate Normal distribution
X_without <- mvrnorm(n=200, mu=c(0,0,0), 
                     Sigma=SigmaX_without)

X_with_positive <- mvrnorm(n=200, mu=c(0,0,0), 
                           Sigma=SigmaX_with_positive)

X_with_negative <- mvrnorm(n=200, mu=c(0,0,0), 
                           Sigma=SigmaX_with_negative)

# and the vector for the error term
error <- rnorm(200)*2



```
---

Which of these estimators would we expect to be biased?

1. The three independent variables belong in the real model and they are correlated -- and we specify the right regression.
```{r}
reg1 <- replicate(10000, ovb_sim(X=X_with_positive, 
                                 e=error,
                                 betas=c(1,2,4,3), 
                                 omitted=FALSE))
```

2. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$

```{r}
reg2 <- replicate(10000, ovb_sim(X=X_with_positive, 
                                 e=error,
                                 betas=c(1,2,4,3), 
                                 omitted=TRUE))
```

---

3. The three independent variables belong in the real model and they are correlated (positive covariance) -- our regression omits $x_3$
```{r}
reg3 <- replicate(10000, ovb_sim(X=X_with_negative, 
                                 e=error,
                                 betas=c(1,2,4,3), 
                                 omitted=TRUE))
```

4. The three independent variables belong in the real model but they are not correlated -- our regression omits $x_3$

```{r}
reg4 <- replicate(10000, ovb_sim(X=X_without, e=error,
                                 betas=c(1,2,4,3),  omitted=TRUE))
```



---
  
Recall $\beta_0=1$, $\beta_1=2$, $\beta_2=4$ and $\beta_3=3$.
  
```{r}

# Average OLS estimate across 10000 simulation runs: 
apply(reg1, 1, mean) # correlated covariates but complete model
apply(reg2, 1, mean) # positively correlated covariates and omitted variable

```

---
  
Recall $\beta_0=1$, $\beta_1=2$, $\beta_2=4$ and $\beta_3=3$.

```{r}

apply(reg3, 1, mean) # negatively correlated covariates and omitted variable
apply(reg4, 1, mean) # non-correlated covariates and omitted variable


```
---

```{r, echo=FALSE, fig.height=5}
plotfunc(coefs=reg1, betas=c(1,2,4,3), omitted=FALSE, 
         maintitle="Complete model")

```

---
  
```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg2, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (positive covariance)")

```

---

```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg3, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (negative covariance)")
```

---
  
```{r, echo=FALSE, fig.height=5}

plotfunc(coefs=reg4, betas=c(1,2,4,3), omitted=TRUE, 
         maintitle="Omitted variable (zero correlation)")

```

Why is this not just centered exactly around the parameter?

